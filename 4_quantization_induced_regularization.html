
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Quantization-Induced Regularization &#8212; Neural Pasta</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Wonderful Minifloats" href="3_wonderful_minifloats.html" />
    <link rel="prev" title="NRLHL PASSTA" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neural Pasta</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    NRLHL PASSTA
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Quantization-Induced Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3_wonderful_minifloats.html">
   Wonderful Minifloats
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2_data_versioning_done_right_pt1.html">
   Data Versioning Done Right.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1_argmax_and_hidden_biases.html">
   Argmax and Hidden Biases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="0_emd_is_just_mse.html">
   EMD Is Just MSE (Kinda)
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/dmtlvn/dmtlvn.github.io/master?urlpath=tree/4_quantization_induced_regularization.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/dmtlvn/dmtlvn.github.io"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/dmtlvn/dmtlvn.github.io/issues/new?title=Issue%20on%20page%20%2F4_quantization_induced_regularization.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/4_quantization_induced_regularization.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Quantization-Induced Regularization
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#formulation">
   Formulation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#integer-quantization">
   Integer Quantization
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#floating-point-quantization">
   Floating-Point Quantization
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implications">
   Implications
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Quantization-Induced Regularization</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Quantization-Induced Regularization
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#formulation">
   Formulation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#integer-quantization">
   Integer Quantization
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#floating-point-quantization">
   Floating-Point Quantization
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implications">
   Implications
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="quantization-induced-regularization">
<h1>Quantization-Induced Regularization<a class="headerlink" href="#quantization-induced-regularization" title="Permalink to this headline">#</a></h1>
<p>During our <a class="reference internal" href="3_wonderful_minifloats.html"><span class="doc std std-doc">minifloat adventure</span></a> it became clear that low-precision arithmetic produces quantization noise so strong that it must change statistical properties of ML models. This led me to a thought: in the simplest case of the linear regression how this quantization noise would affect the model? Here’s a small note on that. <em>(Hint is in the title)</em>.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="formulation">
<h1>Formulation<a class="headerlink" href="#formulation" title="Permalink to this headline">#</a></h1>
<p>In general, modern computers cannot represent <em>true</em> real numbers in all their Platonic nature, they are always quantized. Quantization makes a true value <span class="math notranslate nohighlight">\(x\)</span> unobservable. A quantized value <span class="math notranslate nohighlight">\(q\)</span> is not a real number, it is just a label for an infinite-sized equivalence class consisting of all numbers from some neighborhood <span class="math notranslate nohighlight">\([q-a, q+b)\)</span>. When provided a <span class="math notranslate nohighlight">\(q\)</span> the true number can be anything from this range and without any prior knowledge any number can result in <span class="math notranslate nohighlight">\(q\)</span> class equally likely. We can model this uncertainty by a random variable <span class="math notranslate nohighlight">\(\varepsilon \sim U[-a, b]\)</span> independent of <span class="math notranslate nohighlight">\(q\)</span>:</p>
<div class="math notranslate nohighlight">
\[x = q + \varepsilon\]</div>
<p>Parameters <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> depend on the quantization scheme used. One may point out that quantization isn’t random, and it surely isn’t. But the point is not in the randomness of values but in the <em>distribution</em> of the quantization errors: without any prior knowledge taking a random number from the equivalent class represented by <span class="math notranslate nohighlight">\(q\)</span> results in a uniform distribution.</p>
<p>A linear regression is formulated as:</p>
<div class="math notranslate nohighlight">
\[ y = x^T w,\quad (x, y) \in D \]</div>
<p>Modifying it to account for quantization noise would produce the following equation:</p>
<div class="math notranslate nohighlight">
\[ y + \varepsilon_y = \left( x + \varepsilon_x \right)^T w \]</div>
<p>Parameter vector <span class="math notranslate nohighlight">\(w\)</span> is unmodified because it is unobserved and estimated based on data, which is quantized. So let’s see what this means for integer and floating-point quantization.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="integer-quantization">
<h1>Integer Quantization<a class="headerlink" href="#integer-quantization" title="Permalink to this headline">#</a></h1>
<p>Under integer quantization <span class="math notranslate nohighlight">\(\varepsilon \sim U\left[-\frac{d}{2}, \frac{d}{2}\right]\)</span> where <span class="math notranslate nohighlight">\(d\)</span> is a quantization step. We can notice that:</p>
<div class="math notranslate nohighlight">
\[ \mathbb{E}[\varepsilon] = 0 \]</div>
<div class="math notranslate nohighlight">
\[ \sigma_\varepsilon^2 = \frac{1}{12} d^2 \]</div>
<p>Now recall a least-squared loss function for linear regression:</p>
<div class="math notranslate nohighlight">
\[ L(w) = \mathbb{E}_{D} \left[ \left( y + \varepsilon_y - \left( x + \varepsilon_x \right)^T w \right)^2 \right] \]</div>
<p>Let’s perform some algebra real quick:</p>
<div class="math notranslate nohighlight">
\[\begin{split} z^2 = \left( \left( y - x^T w \right) + \varepsilon_y - \varepsilon_x^T w \right)^2 = \\
    = \left( y - x^T w \right)^2 + \varepsilon_y^2 + w^T \varepsilon_x \varepsilon_x^T w + 2 \left( y - x^T w \right) \varepsilon_y  - 2 \left( y - x^T w \right) \varepsilon_x^T w - 2 \varepsilon_y \varepsilon_x^T w \end{split}\]</div>
<p>Now let’s take and expectation with respect to <span class="math notranslate nohighlight">\(\varepsilon\)</span> and use the fact <span class="math notranslate nohighlight">\(\varepsilon_x\)</span> and <span class="math notranslate nohighlight">\(\varepsilon_y\)</span> are independent of each other and anything else:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \mathbb{E}_{\varepsilon} \left[ z^2 \right] 
    = \left( y - x^T w \right)^2 
        + \mathbb{E} \left[ \varepsilon_y^2 \right]
        + w^T \mathbb{E} \left[ \varepsilon_x \varepsilon_x^T \right] w + \\
        + 2 \left( y - x^T w \right) \mathbb{E} \left[ \varepsilon_y \right] 
        - 2 \left( y - x^T w \right) \mathbb{E} \left[ \varepsilon_x \right]^T w 
        - 2 \mathbb{E} \left[ \varepsilon_y \right] \mathbb{E} \left[ \varepsilon_x \right]^T w = \\
    = \left( y - x^T w \right)^2 + \sigma^2 + \sigma^2 w^T w = \left( y - x^T w \right)^2 + \sigma^2 + \sigma^2 \| w \|^2 \end{split}\]</div>
<p>Putting that back into the loss function produces:</p>
<div class="math notranslate nohighlight">
\[ L(w) = \mathbb{E}_{D} \left[ z^2 \right] = \mathbb{E}_{D} \left[ \left( y - x^T w \right)^2 \right] + \sigma^2 \| w \|^2 + \sigma^2 \]</div>
<p>Adding a constant to a loss function doesn’t change the minimum, so the <span class="math notranslate nohighlight">\(\sigma^2\)</span> term can be dropped:</p>
<div class="math notranslate nohighlight">
\[ L(w) = \mathbb{E}_{D} \left[ \left( y - x^T w \right)^2 \right] + \sigma^2 \| w \|^2 \]</div>
<p>Which is the same least-squares problem but with <span class="math notranslate nohighlight">\(L_2\)</span> regularization with regularization parameter:</p>
<div class="math notranslate nohighlight">
\[ \lambda = \frac{1}{12} d^2 \]</div>
<p>This is kinda expected as <span class="math notranslate nohighlight">\(L_2\)</span> regularization is equivalent to adding zero-mean noise to the regressors, which we precisely did by our quantization scheme.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="floating-point-quantization">
<h1>Floating-Point Quantization<a class="headerlink" href="#floating-point-quantization" title="Permalink to this headline">#</a></h1>
<p>Quantization noise distribution under FP quantization is a bit more complex. Given a floating point representation <span class="math notranslate nohighlight">\(x = M \cdot 2^E\)</span> the distribution is <span class="math notranslate nohighlight">\(\varepsilon \sim U \left[ -\frac{a}{2}, \frac{a}{2} \right]\)</span>, where:</p>
<div class="math notranslate nohighlight">
\[ a = p \cdot 2^{\lceil \log_2 x \rceil} \]</div>
<p>Here <span class="math notranslate nohighlight">\(p\)</span> is the minimal representable normal FP number. This distribution has zero mean and variance <span class="math notranslate nohighlight">\(\sigma^2 = \frac{1}{12} p^2 4^{\lceil \log_2 x \rceil}\)</span></p>
<p>This is because the absolute error of a floating point number doubles with each power of two. This exact formulation is unwieldy because of its nonlinear dependence on <span class="math notranslate nohighlight">\(x\)</span>, which will complicate further calculations quite a bit. Instead, we’re gonna pretend that <span class="math notranslate nohighlight">\( \lceil \log_2 x \rceil \approx \log_2 x \)</span> and use the following approximation:</p>
<div class="math notranslate nohighlight">
\[ a \approx p \cdot x \]</div>
<p>Now we can decompose this distribution as <span class="math notranslate nohighlight">\(\varepsilon = x \cdot \delta\)</span>, where <span class="math notranslate nohighlight">\( \delta \sim U \left[ -\frac{p}{2}, \frac{p}{2} \right]\)</span> and is now independent of <span class="math notranslate nohighlight">\(x\)</span>. This transforms our quantized linear regression formula into this:</p>
<div class="math notranslate nohighlight">
\[ y + y \delta_y = \left( x + x \delta_x \right)^T w \]</div>
<p>The loss function now looks like this:</p>
<div class="math notranslate nohighlight">
\[ L(w) = \mathbb{E}_{D} \left[ \left( y + y \delta_y - \left( x + x \delta_x \right)^T w \right)^2 \right] \]</div>
<p>We’re gonna expand the brackets and take the expectation with respect to <span class="math notranslate nohighlight">\(\delta\)</span> the same as before, so I leave it to you as an exercise. After doing all that we obtain the following:</p>
<div class="math notranslate nohighlight">
\[ \mathbb{E}_{\varepsilon} \left[ z^2 \right] = \left( y - x^T w \right)^2 + \sigma^2 y^2 + \sigma^2 w^T x x^T w \]</div>
<p>Putting that back into the loss function gives:</p>
<div class="math notranslate nohighlight">
\[\begin{split} L(w) = \mathbb{E}_{D} \left[ z^2 \right] 
    = \mathbb{E}_{D} \left[ \left( y - x^T w \right)^2 \right] + \sigma^2 \mathbb{E}_{D} \left[ y^2 \right] + \sigma^2 w^T \mathbb{E}_{D} \left[ x x^T \right] w = \\
    = \mathbb{E}_{D} \left[ \left( y - x^T w \right)^2 \right] + \sigma^2 \mathbb{E}_{D} \left[ y^2 \right] + \sigma^2 w^T \Sigma_X w \end{split}\]</div>
<p>Notice that we can always transform our problem by standardizing <span class="math notranslate nohighlight">\(y\)</span> as well as whitening <span class="math notranslate nohighlight">\(x\)</span>, so <span class="math notranslate nohighlight">\(\mathbb{E}_{D} \left[ y^2 \right] = 1\)</span> and <span class="math notranslate nohighlight">\(\Sigma_X = I\)</span>. And again, by noticing that addition of a constant doesn’t change the minimum of a function, we get:</p>
<div class="math notranslate nohighlight">
\[ L(w) = \mathbb{E}_{D} \left[ \left( y - x^T w \right)^2 \right] + \sigma^2 \| w \|^2 \]</div>
<p>So again we get an <span class="math notranslate nohighlight">\(L_2\)</span>-regularized model but under the assumption that the data is whitened and normalized. The regularization strength in this case bacomes:</p>
<div class="math notranslate nohighlight">
\[ \lambda = \frac{1}{12} p^2 \]</div>
<p>I was looking for ways to verify these results, and the <em>proper</em> way to do this is to fit two linear models - the one on the “clean” data with <span class="math notranslate nohighlight">\(L_2\)</span> prior, and another one on the noisy data without <span class="math notranslate nohighlight">\(L_2\)</span> prior - and run a test for coefficient equality. But unfortunately statistical tests for equality of linear regression coefficients I managed to find aren’t suited for this kind of a problem. Wald test compares regression coefficients to a hypothesized value, not another parameter. Chow test requires the same model to be fitted on multiple data groups but we have different ones. Z-test requires independent data samples for two models, but our data is definitely not independent. So I decided to just brute-force my way out of this by running extensive simulations to demonstrate that the difference between two parameter vectors converges to zero, which it does indeed:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">DATA_ITER</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">DATA_SIZE</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">DATA_DIM</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">NOISE_ITER</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">NOISE_SIGMA</span> <span class="o">=</span> <span class="mf">1e-1</span>
<span class="n">ALPHA</span> <span class="o">=</span> <span class="mi">1000</span> <span class="o">*</span> <span class="n">NOISE_SIGMA</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">whiten</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">y</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">average_model_over_noise</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">coef</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">X</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">estimate</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">coef_</span>
        <span class="n">coef</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">estimate</span><span class="p">)</span>
    <span class="n">coef</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">coef</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">coef</span>

<span class="n">error_norm</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">DATA_ITER</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">DATA_SIZE</span><span class="p">,</span> <span class="n">DATA_DIM</span><span class="p">)</span>
    <span class="n">coef_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">ALPHA</span><span class="p">,</span> <span class="n">fit_intercept</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">coef_</span>
    <span class="n">coef_noise</span> <span class="o">=</span> <span class="n">average_model_over_noise</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">NOISE_SIGMA</span><span class="p">,</span> <span class="n">NOISE_ITER</span><span class="p">)</span>
    <span class="n">error_norm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">coef_reg</span> <span class="o">-</span> <span class="n">coef_noise</span><span class="p">))</span>
    
<span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">error_norm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5.690064795791926e-05
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="implications">
<h1>Implications<a class="headerlink" href="#implications" title="Permalink to this headline">#</a></h1>
<p>This implicit regularization doesn’t manifest itself unless there are some extreme conditions. For example, even standard FP32 numbers produce this effect, but it is really tiny. The minimal normal FP32 number is ~10<sup>-38</sup>, which makes the regularization parameter about ~10<sup>-77</sup> give or take. But for half-precision numbers it would be ~10<sup>-10</sup> already. Where it becomes really prominent is at integer quantization over large ranges and low-precision FP quantization.</p>
<p>Let’s consider an 8-bit integer quantizer with range <span class="math notranslate nohighlight">\(R\)</span>. The quantization step is equal to <span class="math notranslate nohighlight">\(d = \frac{1}{256} R\)</span> and regularization strength therefore being <span class="math notranslate nohighlight">\(\lambda = \frac{1}{3072} R^2\)</span>. So even a range of 1 will have regularization strength comparable to that used in practice.</p>
<p>The same is true for low-precision floating-point numbers. An FP8 number (in a reasonable configuration) would induce regularization of order ~10<sup>-5</sup> which is on a lower side of values used in many papers. But FP4 numbers raise it to ~0.01 which is kinda significant.</p>
<p>It should be noted that at such low precision the approximation for the quantization noise distribution we made earlier is very inaccurate so the exact numbers are almost certainly different. But as a sanity check I ran a quick test by fitting a model to “true” FP32 data and then to the same data but quantized down to FP4 precision. I then compared the coefficient vectors between the two and observed a consistent 1.5% decrease of the norm for the quantized version which is exactly what <span class="math notranslate nohighlight">\(L_2\)</span> regularization does.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="conclusion">
<h1>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">#</a></h1>
<p>Today we’ve explored the impact of an aggressive quantization on “learning” properties of a linear regression. We’ve established analytically that a linear model fitted to a quantized data is <span class="math notranslate nohighlight">\(L_2\)</span>-regularized in relation to the <em>“true”</em> data, meaning that the norm of the coefficient vector for a quantized model is smaller. We’ve also established a relation between quantization accuracy and regularization strength, which while being mostly negligible can become pretty significant at extremely low precisions.</p>
<p>Unfortunately, it is unclear at the moment if there’s a regularization effect of quantization in a deep learning setting, where all these FP8s, FP4s and INT8s make more sense, so I guess it’s a topic for a future post. Have a good night.</p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><strong>NRLHL PASSTA</strong></p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3_wonderful_minifloats.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Wonderful Minifloats</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Dmytro Levin<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>