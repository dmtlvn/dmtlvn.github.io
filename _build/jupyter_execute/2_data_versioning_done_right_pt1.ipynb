{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0718a404",
   "metadata": {},
   "source": [
    "# Data Versioning Done Right.\n",
    "\n",
    "*Apr 6, 2023*\n",
    "\n",
    "---------------------------------------------------------\n",
    "\n",
    "Reproducibility is one of the most important problems in mid- to large-scale ML projects. As the project grows, it accumulates dependencies, legacy and unclear use cases and keeping all that mess organized sometimes requires big changes to development practices and infrastructure. Reproducibility is a multifaceted issue with many aspects to it, but today I want to talk particularly about data versioning.\n",
    "\n",
    "Data versioning is a practice/tool which allows you to keep your datasets and training code in sync. As the project grows so does the dataset. The training data is often composed of multiple open-source and private dataset which are stored in different formats, serve different purposes and require revision every now and then. This makes it really easy to forget which data a particular version of your model was trained on. When you add preprocessing into the mix, things get even more complicated. Data preprocessing is often a time-consuming calculation, and I've seen how people (and me as well) follow this anti-pattern of writing a standalone data preprocessing script, which dumps results into some folder and then use that folder as a \"dataset\" for the model training. It is pretty bad for reproducibility on itself but it can also lead to even bigger problems later on.\n",
    "\n",
    "> **Example:** Suppose your dataset consists of photos of people, but you need only their faces. So you run your photos through a face detector, crop faces, resize them to a desired size and save to a directory. Now imagine that you changed parameters of the face detector to make wider face crops. The image size stays the same and two folders of generated crops (tight and wide) will be indistinguishable for a training code on their own, so technically you can use wrong data. To prevent that you need to manually check the version of the data used, and what is done manually is prone to mistakes. As your preprocessing pipeline becomes bigger it will require more and more manual checking. This gets even worse when multiple people work on the project from different machines, as they may have different folder structure and just pass wrong paths to the training code. If two people conduct independent experiments and one of them unintentionally uses the wrong data, it will cause conflicting results which are hard to debug and may lead research into the wrong direction and cost a lot of time.\n",
    "\n",
    "Besides the obvious problems with this approach there's a hidden one. When a team eventually decides to implement data versioning into their project they can easily confuse these preprocessed folders with the actual dataset and try to apply data versioning to them instead. Conceptually there's nothing wrong with it: that folder is data which can change, the changes must be tracked, and there's a tool for tracking changes in the data. The problem, however, hides in a way data versioning tools perform tracking. So let's talk about that a bit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56954f4",
   "metadata": {},
   "source": [
    "# Data Versioning\n",
    "\n",
    "There are several such solutions for data versioning out there, which share a lot of functionality. I myself mostly have to deal with [DVC](https://dvc.org/) so I will be using it as an example.\n",
    "\n",
    "\n",
    "Data versioning systems like DVC provide two main features along other bells and whistles:\n",
    "\n",
    "1. Detect changes in a data folder using some sort of hashing and record these changes into a register.\n",
    "2. Provide some storage management to sync these registers between different providers: local storage, S3, GCS, databases etc.\n",
    "\n",
    "But because datasets are mostly stored as binaries, there's no way for a DVC to calculate meaningful diffs between files. Everything it can do is to compare hashes and create a copy of a file if hashes don't match. All those copies are stored in a remote data repo separately from the code (in a S3 bucket, for example) and are linked to your project via a registry file added to your normal Git repo. You can probably guess where this is going - we can only have so many versions of our dataset before our storage blows up. This is crucial when we try to manage automatically generated data using DVC. Any tiny change to the data processing code completely messes up hashes so DVC creates a full copy of the data on the remote storage. \n",
    "\n",
    "> **Example:** On one of my projects a dataset contained 270GB of photos of people. Each change to a preprocessing codes will chew away 270B at a time and after only 10 changes you'll get a 3TB storage bill for what can be described by a handful of bytes in Git. And it is hard to impress anybody with a 270GB dataset nowadays, when models like Stable Diffusion are trained on 1000x  larger data.\n",
    "\n",
    "This poses a dilemma: \n",
    "- If your dataset experiences frequent changes, this suggests using data versioning, which is very storage inefficient. \n",
    "- If your dataset is stable, then you don't need data versioning - there will be just a single commit.\n",
    "\n",
    "But the truth is that in cases like that you actually don't need data versioning. You need caching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191a477",
   "metadata": {},
   "source": [
    "# Data Caching\n",
    "\n",
    "The main reason to create a folder with preprocessed data is to speed up your work: you memorize the outputs of your preprocessing code, so when you need them again, you can just read them from disk instead of recomputing. This is what caching does. But the interaction with the cache is different, and this is what causes problems. Here's an example of the correct usage of caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5778728c",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1832748186.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [1], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    x = process_data(69)  # first use - results are computed\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "@cache\n",
    "def process_data(sample):\n",
    "    # some transformation of the sample\n",
    "    # ...\n",
    "\n",
    "x = process_data(69)  # first use - results are computed\n",
    "use_data(x)\n",
    "\n",
    "# other stuff is done here\n",
    "# ... \n",
    "\n",
    "x = process_data(69)  # second use - results are read from cache\n",
    "use_data(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281abc5",
   "metadata": {},
   "source": [
    "Notice how data preprocessing is called both times. This ensures that `process_data` is always in sync with `use_data` - they are called together despite the state of the cache. But this is how caching is done in our folder-with-files case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c4952c",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data\u001b[49m(\u001b[38;5;241m69\u001b[39m) \n\u001b[0;32m      2\u001b[0m save_to_cache_folder(x)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# a possible break in execution\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# ... \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'process_data' is not defined"
     ]
    }
   ],
   "source": [
    "x = process_data(69) \n",
    "save_to_cache_folder(x)\n",
    "\n",
    "# a possible break in execution\n",
    "# ... \n",
    "\n",
    "x = load_from_cache_folder()\n",
    "use_data(x)  # first use - results are read from cache\n",
    "\n",
    "x = load_from_cache_folder()\n",
    "use_data(x)  # second use - results are read from cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd314de",
   "metadata": {},
   "source": [
    "Here `process_data` and `use_data` are not in sync and all sorts of bad things can happen. If we'd change our code to the correct one - with redundant calls to the `process_data` function, then we would achieve two things:\n",
    "\n",
    "1. We would ensure that our preprocessing code does not diverge from the training code, because it gets called every single time and not only at the start of the project\n",
    "2. Any change to the preprocessed data would be reflected in the preprocessing code, which is managed by Git and takes no storage compared to a complete new dump of data created by DVC.\n",
    "\n",
    "Having that the only data changes which would go to a DVC repo would be some manual changes - the only changes which cannot be stored in any other way. This way our DVC repo would have minimal size without compromises to traceability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682f3ca",
   "metadata": {},
   "source": [
    "# Cache Design\n",
    "\n",
    "Unfortunately, DVC does not provide tooling necessary for such data caching. So let's think of potential use cases and design our cache more accurately\n",
    "\n",
    "> **Requirement #1:** Cache must be persistent between multiple runs of the training pipeline on the same working machine, restarts of the working machine and even multiple working machines.\n",
    "\n",
    "The last one is needed so the different people can reuse the same dataset with costly recomputation. These requirements suggest a disk-based cache. Its read times are on par with plain files; it is essentially a folder with files but smarter, so it lives as long as storage does and it can be synced between multiple machines using various tools.\n",
    "\n",
    "> **Requirement #2:** Any change of the cacheable function which changes its outputs must lead to a recomputation of the cached value.\n",
    "\n",
    "This requirement is trivially satisfied in memory-based caches, because cacheable functions usually don't change in runtime. But for a disk-based cache there's no such guarantees. More so, we build this cache exactly to catch code changes and bind them to data. This validation mechanism needs to compare a value stored in the cache to a result recomputed during every run. This requires access to raw function arguments. It can be a problem. Arguments to a cacheable function can be tensors or arrays which take up a lot of memory. So here follows the next requirement: \n",
    "\n",
    "> **Requirement #3:** Cache keys must have minimal memory footprint.\n",
    "\n",
    "In other circumstances we could use hashes, but not in our case. We can solve this issue, however. Because DVC takes care of raw data, we can pass paths to files with raw data to our processing functions instead of actual data, so the function signatures can be stored efficiently. \n",
    "\n",
    "> **Requirement #4:** Cache must have multiple levels for heavy data blobs and lightweight metadata records.\n",
    "\n",
    "A data record may not be a unitary blob of data but have multiple fields. We can roughly divide all the extracted data into four groups:\n",
    "\n",
    "|                    | **Compute - Slow** | **Compute - Fast** |\n",
    "|:-------------------|:-------------------|:-------------------|\n",
    "| **Memory - Low**   | Hard to compute, easy to store. <br> Examples: *class indices, <br> embeddings, descriptors etc.* <br><br> Cached. | Info extracted from function arguments. <br> Examples: *IDs, tags, labels extracted <br> from file paths.* <br><br> Not cached |  \n",
    "| **Memory - High**  | Hard to compute, hard to store. <br> Examples: *segmentation masks, <br> features, generated images etc.* <br><br> Cached. | Simple ops on raw data. <br> Examples: *image normalization, <br> resize, flip etc.* <br><br> Not cached |\n",
    "\n",
    "The first group consists of some lightweight objects which are extracted from the raw data using ML models or other slow algorithms. This data is typically used for sampling and/or filtering further in the data pipeline, so we need to be able to prefetch such data from the cache into memory. On the other hand the second group includes data which cannot be fit into memory in large numbers and must be loaded lazily. The data record may consist of both types of data so our cache should support both as well. \n",
    "\n",
    "These are four main requirements to our data preprocessing cache. There are plenty of Python tools for caching but it seems like they fall short in one way or another. I haven't been able to find disk caching tools with appropriate invalidation strategies - most of them use TTL of some kind, which is not what we want, as our cache can potentially live for months. Also two-level caching with lazy reads is kinda a specific requirement, so I haven't been able to find anything like that as well. For this reason we'll resort to writing our own solution. We will definitely face fun engineering challenges there. But that's it for today.\n",
    "\n",
    "Until next time."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}